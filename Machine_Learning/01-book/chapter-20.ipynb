{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "# Create feature\n",
    "features = np.array([[-100.1, 3240.1],[-200.2,-234.1],[5000.5, 150.1],[6000.6, -125.1],[9000.9, -673.1]])\n",
    "# Create scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Transform the feature\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "# Show feature\n",
    "features_standardized"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print mean and standard deviation\n",
    "print(\"Mean:\", round(features_standardized[:,0].mean()))\n",
    "print(\"Standard deviation:\", features_standardized[:,0].std())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "num_words=number_of_features)\n",
    "# Convert movie review data to one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(\n",
    "number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "target_train, # Target vector\n",
    "epochs=3, # Number of epochs\n",
    "verbose=1, # Print description after each epoch\n",
    "batch_size=100, # Number of observations per batch\n",
    "validation_data=(features_test, target_test)) # Test data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# View shape of feature matrix\n",
    "features_train.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 5000\n",
    "# Load feature and target data\n",
    "data = reuters.load_data(num_words=number_of_features)\n",
    "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
    "# Convert feature data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "# One-hot encode target vector to create a target matrix\n",
    "target_train = to_categorical(target_vector_train)\n",
    "target_test = to_categorical(target_vector_test)\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100,\n",
    "activation=\"relu\",\n",
    "input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "target_train, # Target\n",
    "epochs=3, # Three epochs\n",
    "verbose=0, # No output\n",
    "batch_size=100, # Number of observations per batch\n",
    "validation_data=(features_test, target_test)) # Test data Using TensorFlow backend."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# View target matrix\n",
    "target_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_regression(n_samples = 10000,\n",
    "n_features = 3,\n",
    "n_informative = 3,\n",
    "n_targets = 1,\n",
    "noise = 0.0,\n",
    "random_state = 0)\n",
    "# Divide our data into training and test sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "features, target, test_size=0.33, random_state=0)\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=32,\n",
    "activation=\"relu\",\n",
    "input_shape=(features_train.shape[1],)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=32, activation=\"relu\"))\n",
    "# Add fully connected layer with no activation function\n",
    "network.add(layers.Dense(units=1))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"mse\", # Mean squared error\n",
    "optimizer=\"RMSprop\", # Optimization algorithm\n",
    "metrics=[\"mse\"]) # Mean squared error\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "target_train, # Target vector\n",
    "epochs=10, # Number of epochs\n",
    "verbose=0, # No output\n",
    "batch_size=100, # Number of observations per batch\n",
    "validation_data=(features_test, target_test)) # Test data Using TensorFlow backend."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 10000\n",
    "# Load data and target vector from IMDB movie data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "num_words=number_of_features)\n",
    "# Convert IMDB data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "activation=\"relu\",\n",
    "input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "target_train, # Target vector\n",
    "epochs=3, # Number of epochs\n",
    "verbose=0, # No output\n",
    "batch_size=100, # Number of observations per batch\n",
    "validation_data=(features_test, target_test)) # Test data\n",
    "# Predict classes of test set\n",
    "predicted_target = network.predict(features_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# View the probability the first observation is class 1\n",
    "predicted_target[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 10000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "num_words=number_of_features)\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "activation=\"relu\",\n",
    "input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "target_train, # Target\n",
    "epochs=15, # Number of epochs\n",
    "verbose=0, # No output\n",
    "batch_size=1000, # Number of observations per batch\n",
    "validation_data=(features_test, target_test)) # Test data\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history[\"loss\"]\n",
    "test_loss = history.history[\"val_loss\"]\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, \"r--\")\n",
    "plt.plot(epoch_count, test_loss, \"b-\")\n",
    "plt.legend([\"Training Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get training and test accuracy histories\n",
    "training_accuracy = history.history[\"accuracy\"]\n",
    "test_accuracy = history.history[\"val_accuracy\"]\n",
    "plt.plot(epoch_count, training_accuracy, \"r--\")\n",
    "plt.plot(epoch_count, test_accuracy, \"b-\")\n",
    "# Visualize accuracy history\n",
    "plt.legend([\"Training Accuracy\", \"Test Accuracy\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.show();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "num_words=number_of_features)\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "activation=\"relu\",\n",
    "kernel_regularizer=regularizers.l2(0.01),\n",
    "input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "kernel_regularizer=regularizers.l2(0.01),\n",
    "activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "target_train, # Target vector\n",
    "epochs=3, # Number of epochs\n",
    "verbose=0, # No output\n",
    "batch_size=100, # Number of observations per batch\n",
    "validation_data=(features_test, target_test)) # Test data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "num_words=number_of_features)\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "activation=\"relu\",\n",
    "input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "ModelCheckpoint(filepath=\"best_model.h5\",\n",
    "monitor=\"val_loss\",\n",
    "save_best_only=True)]\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "target_train, # Target vector\n",
    "epochs=20, # Number of epochs\n",
    "callbacks=callbacks, # Early stopping\n",
    "verbose=0, # Print description after each epoch\n",
    "batch_size=100, # Number of observations per batch\n",
    "validation_data=(features_test, target_test)) # Test data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "num_words=number_of_features)\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add a dropout layer for input layer\n",
    "network.add(layers.Dropout(0.2, input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network.add(layers.Dropout(0.5))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network.add(layers.Dropout(0.5))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "target_train, # Target vector\n",
    "epochs=3, # Number of epochs\n",
    "verbose=0, # No output\n",
    "batch_size=100, # Number of observations per batch\n",
    "validation_data=(features_test, target_test)) # Test data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "num_words=number_of_features)\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "activation=\"relu\",\n",
    "input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "checkpoint = [ModelCheckpoint(filepath=\"models.hdf5\")]\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "target_train, # Target vector\n",
    "epochs=3, # Number of epochs\n",
    "callbacks=checkpoint, # Checkpoint\n",
    "verbose=0, # No output\n",
    "batch_size=100, # Number of observations per batch\n",
    "validation_data=(features_test, target_test)) # Test data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Number of features\n",
    "number_of_features = 100\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_classification(n_samples = 10000,\n",
    "n_features = number_of_features,\n",
    "n_informative = 3,\n",
    "n_redundant = 0,\n",
    "n_classes = 2,\n",
    "weights = [.5, .5],\n",
    "random_state = 0)\n",
    "# Create function returning a compiled network\n",
    "def create_network():\n",
    "    # Start neural network\n",
    "    network = models.Sequential()\n",
    "    # Add fully connected layer with a ReLU activation function\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(\n",
    "    number_of_features,)))\n",
    "    # Add fully connected layer with a ReLU activation function\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "    # Add fully connected layer with a sigmoid activation function\n",
    "    network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    # Compile neural network\n",
    "    network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "    optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "    metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "    # Return compiled network\n",
    "    return network\n",
    "    \n",
    "# Wrap Keras model so it can be used by scikit-learn\n",
    "neural_network = KerasClassifier(build_fn=create_network,\n",
    "epochs=10,\n",
    "batch_size=100,\n",
    "verbose=0)\n",
    "# Evaluate neural network using three-fold cross-validation\n",
    "cross_val_score(neural_network, features, target, cv=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Number of features\n",
    "number_of_features = 100\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_classification(n_samples = 10000,\n",
    "n_features = number_of_features,\n",
    "n_informative = 3,\n",
    "n_redundant = 0,\n",
    "n_classes = 2,\n",
    "weights = [.5, .5],\n",
    "random_state = 0)\n",
    "# Create function returning a compiled network\n",
    "def create_network(optimizer=\"rmsprop\"):\n",
    "    # Start neural network\n",
    "    network = models.Sequential()\n",
    "    # Add fully connected layer with a ReLU activation function\n",
    "    network.add(layers.Dense(units=16,\n",
    "    activation=\"relu\",\n",
    "    input_shape=(number_of_features,)))\n",
    "    # Add fully connected layer with a ReLU activation function\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "    # Add fully connected layer with a sigmoid activation function\n",
    "    network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    # Compile neural network\n",
    "    network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "    optimizer=optimizer, # Optimizer\n",
    "    metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "    # Return compiled network\n",
    "    return network\n",
    "    \n",
    "# Wrap Keras model so it can be used by scikit-learn\n",
    "neural_network = KerasClassifier(build_fn=create_network, verbose=0)\n",
    "# Create hyperparameter space\n",
    "epochs = [5, 10]\n",
    "batches = [5, 10, 100]\n",
    "optimizers = [\"rmsprop\", \"adam\"]\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(optimizer=optimizers, epochs=epochs, batch_size=batches)\n",
    "# Create grid search\n",
    "grid = GridSearchCV(estimator=neural_network, param_grid=hyperparameters)\n",
    "# Fit grid search\n",
    "grid_result = grid.fit(features, target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# View hyperparameters of best neural network\n",
    "grid_result.best_params_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "# Set that the color channel value will be first\n",
    "K.set_image_data_format(\"channels_first\")\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "# Set image information\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "# Load data and target from MNIST data\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()\n",
    "# Reshape training image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
    "# Reshape test image data into features\n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)\n",
    "# Rescale pixel intensity to between 0 and 1\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255\n",
    "# One-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]\n",
    "# Start neural network\n",
    "network = Sequential()\n",
    "# Add convolutional layer with 64 filters, a 5x5 window, and ReLU activation function\n",
    "network.add(Conv2D(filters=64,\n",
    "kernel_size=(5, 5),\n",
    "input_shape=(channels, width, height),\n",
    "activation='relu'))\n",
    "# Add max pooling layer with a 2x2 window\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "# Add layer to flatten input\n",
    "network.add(Flatten())\n",
    "# # Add fully connected layer of 128 units with a ReLU activation function\n",
    "network.add(Dense(128, activation=\"relu\"))\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "# Train neural network\n",
    "network.fit(features_train, # Features\n",
    "target_train, # Target\n",
    "epochs=2, # Number of epochs\n",
    "verbose=0, # Don't print description after each epoch\n",
    "batch_size=1000, # Number of observations per batch\n",
    "validation_data=(features_test, target_test)) # Data for evaluation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Create image augmentation\n",
    "augmentation = ImageDataGenerator(featurewise_center=True, # Apply ZCA whitening\n",
    "zoom_range=0.3, # Randomly zoom in on images\n",
    "width_shift_range=0.2, # Randomly shift images\n",
    "horizontal_flip=True, # Randomly flip images\n",
    "rotation_range=90) # Randomly rotate\n",
    "# Process all images from the directory 'raw/images'\n",
    "augment_images = augmentation.flow_from_directory(\"raw/images\", # Image folder\n",
    "batch_size=32, # Batch size\n",
    "class_mode=\"binary\", # Classes\n",
    "save_to_dir=\"processed/images\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train neural network\n",
    "network.fit_generator(augment_images,\n",
    "#Number of times to call the generator for each epoch\n",
    "steps_per_epoch=2000,\n",
    "# Number of epochs\n",
    "epochs=5,\n",
    "# Test data generator\n",
    "validation_data=augment_images_test,\n",
    "# Number of items to call the generator\n",
    "# for each test epoch\n",
    "validation_steps=800)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "num_words=number_of_features)\n",
    "# Use padding or truncation to make each observation have 400 features\n",
    "features_train = sequence.pad_sequences(data_train, maxlen=400)\n",
    "features_test = sequence.pad_sequences(data_test, maxlen=400)\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add an embedding layer\n",
    "network.add(layers.Embedding(input_dim=number_of_features, output_dim=128))\n",
    "# Add a long short-term memory layer with 128 units\n",
    "network.add(layers.LSTM(units=128))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "optimizer=\"Adam\", # Adam optimization\n",
    "metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "target_train, # Target\n",
    "epochs=3, # Number of epochs\n",
    "verbose=0, # Do not print description after each epoch\n",
    "batch_size=1000, # Number of observations per batch\n",
    "validation_data=(features_test, target_test)) # Test data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# View first observation\n",
    "print(features_test[0])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}